传统：图文检索(image retrival) ,视觉问答（VQA），视觉推理（Visual reasoning) 视觉蕴含（visual entialment)

只用transformer encoder: ViLT CLIP ALBEF VLMo
encoder decoder一起：BLIP CoCa BEIT v3  PaLI



ViLT:
![[Pasted image 20240312092334.png]]
希望将目标检测器从视觉端拿掉（运算非常大），在ViT之前没有太好的办法，早期(a)VSE只能做简单的模态之间的交互,(c)中ViLBERT, UNITER 发现模态之间的交互是很重要的，将之前(a)中简单点乘的变成了transformer encoder或别的。但是他们都用到了预训练的目标检测器，加更大的模态融合部分，对训练或部署都很困难。当ViT出来后 ViLT应运而生，因为发现在ViT中基于patch的视觉特征和目标检测器中基于bounding box的没太大区别。直接将目标检测器换为了patch embedding 大大降低了运算复杂度（尤其是推理的时候）。但文本只简单tokenize,视觉只patch embedding远远不够，后面的模态融合非常关键。
ViLT:优点:简单易学（embedding之后扔给transformer encoder学习，和nlp没太大区别
缺点：性能不够高，在很多任务比不上c的方法。原因：1）可能数据集bias,或多模态任务需要更强的视觉能力，->我们需要更强的视觉部分。视觉模型应该要比文本模型大，效果才能好，而ViLT文本tokenizer是很好的，视觉端random initialize所以效果不好。2）ViLT推理很快，训练非常慢

CLIP：
![[Pasted image 20240312092916.png]]
通过对比学习，让是一个的图片文本对拉得更近，学到很好的图像文本特征，之后只需要简单点乘就能做多模态任务（尤其对图像文本匹配/检索，但对其他任务性能不够好）

所以应该很像c.
对比学习loss(ITC loss)效果很好，MLM(mask lauguage modeling ,BERT训练方式，遮住一个预判这个词完形填空)，ITM(image text match loss)

![[Pasted image 20240312093557.png]]

ALBEF：

![[Pasted image 20240312093639.png]]

## Motivation&Contribution:
1. 用了目标检测器，这个检测器和文本不是aligned(因为目标检测器是提前训练好的，没有再进行end--to-end训练)，可能扔给encoder后，不好学之间的交互特征，所以提出在fusing 之前 用contrastive loss align上
2. 避免学习noisy web data ,提出momentum distillation ，自训练的去学习（伪标签Pseudo label),采用Coco提出的Momentum model 生成伪标签

结构：
图像：image transformer  文本：BERT
image encoder:
1. sequence length ：（若是224x224) 196 + clstoken ,绿黄色为197\*768,参数用DeiT 

文本：
将BERT劈开，前面6层做文本的编码，后面进行multimodel融合

Momentum model
为了后面进行momentun distillation,并且给ITC loss提供更多的negative 
(和MoCo相同)：右边也有一套训练参数，由左边moving average 得到。将moving average 参数设置非常高（0.995）保证右边model不会那么快的更新，产生的特征更稳定，更稳定的做negative sample,momentum disllation


目标函数：
ITC loss:
CLS 作为全局的token为 768 \*1 ,经过dawn sample和normalization,有了两个特征。
负样本存在一个q里（很大有65536个负样本，由momentum model产生，所以没有gradient)


ITM:二分类任务，判断I T 是否是一个对。对于负样本：加constraint,通过某种方式选最难的负样本，依赖于ITC：将这个图片和同一个batch的文本都算cos similarity,选择除了自己之外最高的文本当作负样本
MLM:
将完形填空的句子 和图片一起通过模型预测完整的句子。
注：完整模型训练，一次iteration用了两次forward,

![[Pasted image 20240312100607.png]]

