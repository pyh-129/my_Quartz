---
quickshare-date: 2024-03-01 21:13:59
quickshare-url: "https://noteshare.space/note/clt8oe9pj395801mw6jn1rygs#9slkYCOglm3Wlhr+HQLtxhki+gXXAQqM54PvvG4v9vs"
---
# ControlNet
## Motivation
1. 希望能用额外的图像特定构图（, edge maps, human pose skeletons, segmentation maps, depth, normals,）
2. 对大型的T2I模型来讲，学习end-to-end条件的控制具有的挑战：
	1. 训练学习特定的条件的训练数据较小，会导致overfitting and catastrophic forgetting
## Contribution
1. 锁定原来参数：
2. 对一部分encoder layer做trainnable copy
3. 两个部分通过zero convolution layer连接（防止有害的噪声干扰diffusion训练）
## Method

![[Pasted image 20240226112350.png]]
$x$:feature map $c$: conditionnal vector
![[Pasted image 20240226112411.png]]
![[Pasted image 20240226112419.png]]
1. 初始zero conv为0，所以$y_c = y$,没有有害的noise可以影响隐藏层
2. trainable copy: 12 encoding blocks,1 moddle block
3. Controlnet结果通过decoder 12个skip connection加在原来网络
4. 注意SD是将512 x 512转为 64x64作为latent images去训练，为了一致需要一个小的网络将条件c也embedd

## Training
1. 用empty string随机替换50%text->增加ControlNet对conditioning image语义识别能力 (??CFG)
2. 一个观察：sudden convergence phenomenon ![[Pasted image 20240227201530.png]]
## Inference
### CFG
![[Pasted image 20240228162258.png]]
conditioning image ：
1. 加在uc和c: 当无prompt时，同时加会移除guidance
2. 加在c: 使得guidance过强，图像过饱和
解决：加在c,并且在加到SD上时根据13个block大小乘上系数，降低image guidance
### 多个controlnet
每个单独训练，最后相加
## 实验
## Ablative study
说明trainable copy和zero conv有效性 ：1）trainable copy换为conv 2)zero换Gaussian
![[Pasted image 20240228164008.png]]

### 评估
+ user study
+ ADE20K : to evaluate the conditioning fidelity.
+ FID,CLIP-score,CLIP aesthetic score: distribution distance

![[Pasted image 20240228164527.png]]
![[Pasted image 20240228164546.png]]


# DreamBooth
new concept learning(subject-driven)
## Motivation
+ 希望生成具有特定物体的图像：虽然T2I具有语义先验，但输出域表达有限（不能生成same subject,不准确).以往受限于global editing,没法fine-grained control

## Contribution
+ fine-tune T2I :
	+ text prompt带有关于特定物体的identifier，使模型学习到先验，这个先验和特定实体绑定。
	+ 为防止lauguage shift: autogenous, class-specific prior preservation loss

## Method
### T2I
c: conditioning vector
![[Pasted image 20240228180146.png]]

### Personalization
1. 相比于GANs的fine-tune易导致overfit和mode-collapse:
> large text-to-image diffusion models seem to excel at integrating new information into their domain without forgetting the prior or overfitting to a small set of training images.

### Prompt
a \[identifier] \[class noun]
### Identifier
需要identifier在LM和diffusion中都具有weak prior.
+ rare identifier: $f (\hat V)$, f: tockenizer; $\hat V$: decoded text
+ 
### Class-specific Prior Preservation Loss
+ Fine-tune Problem：
	+ finetune 可能带来lauguage drift
		+ (注：LM finetune后失去原先的句法语义知识，diffusion也有相似现象“slowly forgets how to generate subjects of the same class as the target subject”)
	+ 减少了diversity
+ 解决上述两问题：loss
	+ 另外用\[class noun]生成数据 $x_{pr}$
	+ ![[Pasted image 20240228182558.png]]
		+ 第二项prior-preservation term:用生成的数据来指导模型，以保持先验信息

## Experiments

评估：
+ subject fidelity:
	+ CLIP-I : average pairwise cosine similarity between CLIP  embeddings of generated and real images(但不能区分高度相似文本的物体)
	+ DINO: 鼓励区分物体独特特征
+ prompt fidelity
	+ CLIP-T: average pairwise cosine similarity between CLIP embeddings of generated and real images
![[Pasted image 20240228183541.png]]
在Imagen上比在SD上好

### 消融实验
+ Prior preservation loss
+ class-prior

### 局限性
+ 没能根据context生成正确背景
	+ 可能这些a weak prior for contexts,或在数据集中这些概念共现概率低
+ context-appearance entanglement：物体跟随背景变化了
+ overfitting to the real images：prompt和training set中图片的setting较像
+ 一些常见的物体更容易学习