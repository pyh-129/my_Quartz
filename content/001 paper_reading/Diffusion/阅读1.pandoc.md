# DDM {#ddm heading="DDM"}

> quote from Yang Song\
> ![Pasted image
> 20240116102851.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240116102851.png){.internal-embed
> touched="true"}

## BG {#bg heading="BG"}

-   generated by latent variable []{.math .math-inline .is-loaded}
-   generally learn low-dim latent representations

### ELBO {#elbo heading="ELBO"}

1.  []{.math .math-inline .is-loaded}: model the latent variable and the
    data, likelihood-based: maximize the likelihood []{.math
    .math-inline .is-loaded}.

2.  ![Pasted image
    20231129170724.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129170724.png){.internal-embed
    touched="true"}![Pasted image
    20231129170731.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129170731.png){.internal-embed
    touched="true"}

3.  推导evidence lower bound:（VLB ELB)

    1.  ![Pasted image
        20231129171149.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129171149.png){.internal-embed
        touched="true"}KL大于0, []{.math .math-inline .is-loaded}: the
        approximate posterior.p(x):likelihood of observed or generated
        data
        当KL趋近于0意味着logp(x)接近于ELBO式。如果减小KL距离，则q接近真实后验分布。但因为我们不知道真实后验，所以KL很难计算。而左侧p(x)与参数[]{.math
        .math-inline .is-loaded}无关，所以optimize
        ELBO就等于使得估计的后验
        接近真实的后验。并且因为KL趋近于0，可以用ELBO估计p(x)

### VAE {#vae heading="VAE"}

![Pasted image
20240223102231.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240223102231.png){.internal-embed
touched="true"}

1.  maximize ELBO
    1.  ![Pasted image
        20231129172419.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129172419.png){.internal-embed
        touched="true"}
        1.  encoder learn []{.math .math-inline .is-loaded}, decoder
            learn []{.math .math-inline .is-loaded}

    -   式子
        -   reconstruction likelihood of
            decoder:确保decoder对latent有效建模
        -   学习到的 z 分布（variational distribution)
            （q(z\|x))和先验相似程度。确保encoder有效建模而不是一个Dirac函数
2.  encoder: 学习一个multi gaussion, 假设prior是standard gaussion:
    1.  ![Pasted image
        20231129173604.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129173604.png){.internal-embed
        touched="true"}
    2.  reconstruction term 可以用monte varlo估计，latents []{.math
        .math-inline .is-loaded} are sampled from []{.math .math-inline
        .is-loaded} .单纯sample是不可导的-\> reparamterization trick
    3.  reparamterization trick: 将r.v.重写做noise
        variable函数，从而化为从一个standard gaussioan
        中sample,能够对参数做gradient descent.(注意 []{.math
        .math-inline .is-loaded} element-wise product)\
        ![Pasted image
        20231129174306.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129174306.png){.internal-embed
        touched="true"}
    4.  VAE中 z dim 小于x-\>学到compact latent vector, 可修改latent
        vector控制生成

### HVAE {#hvae heading="HVAE"}

将前向反向都看作为马尔科夫链： decoding each latent []{.math
.math-inline .is-loaded} only conditions on previous latent []{.math
.math-inline .is-loaded}, 低级依赖高级

![Pasted image
20231129205321.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129205321.png){.internal-embed
touched="true"}\
![Pasted image
20231129205331.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129205331.png){.internal-embed
touched="true"}\
Lower-bound:\
![Pasted image
20231129205414.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129205414.png){.internal-embed
touched="true"}

## Variational Diffusion Models {#variational-diffusion-models heading="Variational Diffusion Models"}

![Pasted image
20231129205937.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129205937.png){.internal-embed
touched="true"}

1.  与HVAE不同
    1.  latent dim = data dim （所以latent 和data 都写作[]{.math
        .math-inline .is-loaded} ）
    2.  at t, the latent encoder is defined as a linear Gaussian model
    3.  The Gaussian parameter of latent encoder vary -\> at final T,
        standard Gaussian
2.  the distribution of each latent variable in the encoder is a
    Gaussian centered around its previous hierarchical latent.
    1.  ![Pasted image
        20231129210136.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231129210136.png){.internal-embed
        touched="true"}
    2.  linear Gaussian parameters: hyperparameters or learned
    3.  ![Pasted image
        20231130091907.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130091907.png){.internal-embed
        touched="true"}![Pasted image
        20231130091914.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130091914.png){.internal-embed
        touched="true"}
    4.  ![Pasted image
        20231130091944.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130091944.png){.internal-embed
        touched="true"}
    5.  ![Pasted image
        20231130092041.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130092041.png){.internal-embed
        touched="true"}
    6.  不同于HVAE, encoder过程没有参数 []{.math .math-inline
        .is-loaded} ,每一步为Gaussian. 我们关心 []{.math .math-inline
        .is-loaded} 以生成新的[]{.math .math-inline .is-loaded}.
3.  ELBO式子
    1.  ![Pasted image
        20231130093329.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130093329.png){.internal-embed
        touched="true"}
        1.  reconstruction:
        2.  prior matching : 不用优化，没有learnable parameters,
            足够大T可以使最后为gaussian, 这一项变为0
        3.  consistency term: 因此主要优化第三个式子, 然而使用Monte
            Carlo涉及两个r.v. []{.math .math-inline .is-loaded} ,
            会变得不稳定（这两个随机变量噪声叠加，最终估计的是加上所有采样，所以variance
            会高）
        4.  由于[]{.math .math-inline .is-loaded}无法求（注：推导
            :[]{.math .math-inline .is-loaded}是 given []{.math
            .math-inline .is-loaded}，如果将[]{.math .math-inline
            .is-loaded}看作 []{.math .math-inline .is-loaded}
            那么后面就是边缘概率的积分）
            利用markov化为另一种形式![Pasted image
            20231130094503.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130094503.png){.internal-embed
            touched="true"}![Pasted image
            20231130094516.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130094516.png){.internal-embed
            touched="true"}\
            1. 最后一项：denosing matching. 希望denosing transition step
            []{.math .math-inline .is-loaded} 和 ground-truth denosing
            transition step []{.math .math-inline
            .is-loaded}越接近越好。其中在HVAE由于本身encoder
            learning复杂因此[]{.math .math-inline
            .is-loaded}不好计算，而在VDM中可以利用gaussian性质简化：\
            1. ![Pasted image
            20231130095509.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130095509.png){.internal-embed
            touched="true"} 这样可以用markov, 只需要找到[]{.math
            .math-inline .is-loaded}就可以。\
            2. ![Pasted image
            20231130095345.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130095345.png){.internal-embed
            touched="true"}![Pasted image
            20231130095444.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130095444.png){.internal-embed
            touched="true"}\
            3. []{.math .math-inline .is-loaded}最终化为![Pasted image
            20231130095816.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130095816.png){.internal-embed
            touched="true"}\
            2. 由于 希望denosing transition step []{.math .math-inline
            .is-loaded} 和 ground-truth denosing transition step
            []{.math .math-inline .is-loaded}越接近越好，可以把[]{.math
            .math-inline .is-loaded} 建模为Gaussian :![Pasted image
            20231130100706.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130100706.png){.internal-embed
            touched="true"}, mean确定后参数确定，从而variance确定\
            1. ![Pasted image
            20231130100921.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130100921.png){.internal-embed
            touched="true"}![Pasted image
            20231130100935.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130100935.png){.internal-embed
            touched="true"} []{.math .math-inline .is-loaded} 由 neural
            network给出\
            2. ![Pasted image
            20231130101050.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130101050.png){.internal-embed
            touched="true"}\
            3. ![Pasted image
            20231130101105.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130101105.png){.internal-embed
            touched="true"}\
            4. 最后是minimize expectations over all timesteps![Pasted
            image
            20231130101225.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130101225.png){.internal-embed
            touched="true"}
4.  Learning noise parameters
    1.  ![Pasted image
        20231130101848.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130101848.png){.internal-embed
        touched="true"}由前面推导得SNR，SNR越小Noise越大，希望t增加snr单调减少。
    2.  最终可化简为![Pasted image
        20231130102053.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102053.png){.internal-embed
        touched="true"}![Pasted image
        20231130102103.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102103.png){.internal-embed
        touched="true"}
    3.  希望构建nn去model snr, 表达为![Pasted image
        20231130102148.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102148.png){.internal-embed
        touched="true"}
5.  noise形式
    1.  带入![Pasted image
        20231130102716.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102716.png){.internal-embed
        touched="true"}到 []{.math .math-inline .is-loaded} , 得![Pasted
        image
        20231130102741.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102741.png){.internal-embed
        touched="true"}
    2.  带入KL式![Pasted image
        20231130102803.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130102803.png){.internal-embed
        touched="true"}
    3.  表示predict x0相当于predict noise.(一些表明predict noise
        performance更好)
6.  形式
    1.  Tweedie's Formula : \"states that the true mean of an
        exponential family distribution, given samples drawn from it,
        can be estimated by the maximum likelihood estimate of the
        samples (aka empirical mean) plus some correction term involving
        the score of the estimate.\" 矫正项考虑到sample分布情况![Pasted
        image
        20231130103552.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103552.png){.internal-embed
        touched="true"}
    2.  ![Pasted image
        20231130103710.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103710.png){.internal-embed
        touched="true"}![Pasted image
        20231130103727.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103727.png){.internal-embed
        touched="true"}
    3.  带入[]{.math .math-inline .is-loaded} 推出:
        1.  ![Pasted image
            20231130103834.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103834.png){.internal-embed
            touched="true"}![Pasted image
            20231130103849.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103849.png){.internal-embed
            touched="true"}
    4.  ![Pasted image
        20231130103917.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103917.png){.internal-embed
        touched="true"}![Pasted image
        20231130103929.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130103929.png){.internal-embed
        touched="true"}[]{.math .math-inline .is-loaded} is a nn learns
        to predict the score function []{.math .math-inline .is-loaded}
    5.  和原来[]{.math .math-inline .is-loaded}
        关系？：可见多一个scale项，score function
        指导我们如何在数据空间中移动以优化（最大化）目标函数（这里是log
        p）的作用，intuitively，是添加source noise 的反方向
        1.  ![Pasted image
            20231130104036.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020231130104036.png){.internal-embed
            touched="true"}

### DDPM {#ddpm heading="DDPM"}

#### 扩散 {#扩散 heading="扩散"}

::: {.math .math-block .is-loaded}
:::

[]{.math .math-block .is-loaded}[]{.math .math-block .is-loaded}[]{.math
.math-block .is-loaded}

#### 反向 {#反向 heading="反向"}

[]{.math .math-block .is-loaded} []{.math .math-block
.is-loaded}[]{.math .math-block .is-loaded}

![Pasted image
20240223110641.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240223110641.png){.internal-embed
touched="true"}\
注：

# ControlNet {#controlnet heading="ControlNet"}

## Motivation {#motivation heading="Motivation"}

1.  希望能用额外的图像特定构图（, edge maps, human pose skeletons,
    segmentation maps, depth, normals,）
2.  对大型的T2I模型来讲，学习end-to-end条件的控制具有的挑战：
    1.  训练学习特定的条件的训练数据较小，会导致overfitting and
        catastrophic forgetting

## Contribution {#contribution heading="Contribution"}

1.  锁定原来参数：
2.  对一部分encoder layer做trainnable copy
3.  两个部分通过zero convolution
    layer连接（防止有害的噪声干扰diffusion训练）

## Method {#method heading="Method"}

![Pasted image
20240226112350.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240226112350.png){.internal-embed
touched="true"}\
[]{.math .math-inline .is-loaded}:feature map []{.math .math-inline
.is-loaded}: conditionnal vector\
![Pasted image
20240226112411.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240226112411.png){.internal-embed
touched="true"}\
![Pasted image
20240226112419.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240226112419.png){.internal-embed
touched="true"}

1.  初始zero conv为0，所以[]{.math .math-inline
    .is-loaded},没有有害的noise可以影响隐藏层
2.  trainable copy: 12 encoding blocks,1 moddle block
3.  Controlnet结果通过decoder 12个skip connection加在原来网络
4.  注意SD是将512 x 512转为 64x64作为latent
    images去训练，为了一致需要一个小的网络将条件c也embedd

## Training {#training heading="Training"}

1.  用empty string随机替换50%text-\>增加ControlNet对conditioning
    image语义识别能力 (??CFG)
2.  一个观察：sudden convergence phenomenon ![Pasted image
    20240227201530.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240227201530.png){.internal-embed
    touched="true"}

## Inference {#inference heading="Inference"}

### CFG {#cfg heading="CFG"}

![Pasted image
20240228162258.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228162258.png){.internal-embed
touched="true"}\
conditioning image ：

1.  加在uc和c: 当无prompt时，同时加会移除guidance
2.  加在c: 使得guidance过强，图像过饱和\
    解决：加在c,并且在加到SD上时根据13个block大小乘上系数，降低image
    guidance

### 多个controlnet {#多个controlnet heading="多个controlnet"}

每个单独训练，最后相加

## 实验 {#实验 heading="实验"}

## Ablative study {#ablative-study heading="Ablative study"}

说明trainable copy和zero conv有效性 ：1）trainable copy换为conv
2)zero换Gaussian\
![Pasted image
20240228164008.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228164008.png){.internal-embed
touched="true"}

### 评估 {#评估 heading="评估"}

-   user study
-   ADE20K : to evaluate the conditioning fidelity.
-   FID,CLIP-score,CLIP aesthetic score: distribution distance

![Pasted image
20240228164527.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228164527.png){.internal-embed
touched="true"}\
![Pasted image
20240228164546.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228164546.png){.internal-embed
touched="true"}

# DreamBooth {#dreambooth heading="DreamBooth"}

new concept learning(subject-driven)

## Motivation {#motivation-1 heading="Motivation"}

-   希望生成具有特定物体的图像：虽然T2I具有语义先验，但输出域表达有限（不能生成same
    subject,不准确).以往受限于global editing,没法fine-grained control

## Contribution {#contribution-1 heading="Contribution"}

-   fine-tune T2I :
    -   text
        prompt带有关于特定物体的identifier，使模型学习到先验，这个先验和特定实体绑定。
    -   为防止lauguage shift: autogenous, class-specific prior
        preservation loss

## Method {#method-1 heading="Method"}

![Pasted image
20240301204445.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240301204445.png){.internal-embed
touched="true"}

### T2I {#t2i heading="T2I"}

c: conditioning vector\
![Pasted image
20240228180146.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228180146.png){.internal-embed
touched="true"}

### Personalization {#personalization heading="Personalization"}

1.  相比于GANs的fine-tune易导致overfit和mode-collapse:

    > large text-to-image diffusion models seem to excel at integrating
    > new information into their domain without forgetting the prior or
    > overfitting to a small set of training images.

### Prompt {#prompt heading="Prompt"}

a \[identifier\] \[class noun\]

### Identifier {#identifier heading="Identifier"}

需要identifier在LM和diffusion中都具有weak prior.

-   rare identifier: []{.math .math-inline .is-loaded}, f: tockenizer;
    []{.math .math-inline .is-loaded}: decoded text
-   

### Class-specific Prior Preservation Loss {#class-specific-prior-preservation-loss heading="Class-specific Prior Preservation Loss"}

-   Fine-tune Problem：
    -   finetune 可能带来lauguage drift
        -   (注：LM
            finetune后失去原先的句法语义知识，diffusion也有相似现象"slowly
            forgets how to generate subjects of the same class as the
            target subject")
    -   减少了diversity
-   解决上述两问题：loss
    -   另外用\[class noun\]生成数据 []{.math .math-inline .is-loaded}
    -   ![Pasted image
        20240228182558.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228182558.png){.internal-embed
        touched="true"}
        -   第二项prior-preservation
            term:用生成的数据来指导模型，以保持先验信息

## Experiments {#experiments heading="Experiments"}

评估：

-   subject fidelity:
    -   CLIP-I : average pairwise cosine similarity between CLIP
        embeddings of generated and real
        images(但不能区分高度相似文本的物体)
    -   DINO: 鼓励区分物体独特特征
-   prompt fidelity\
    + CLIP-T: average pairwise cosine similarity between CLIP embeddings
    of generated and real images\
    ![Pasted image
    20240228183541.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240228183541.png){.internal-embed
    touched="true"}\
    在Imagen上比在SD上好

### 消融实验 {#消融实验 heading="消融实验"}

-   Prior preservation loss
-   class-prior
-   

### 局限性 {#局限性 heading="局限性"}

-   没能根据context生成正确背景
    -   可能这些a weak prior for contexts,或在数据集中这些概念共现概率低
-   context-appearance entanglement：物体跟随背景变化了
-   overfitting to the real images：prompt和training
    set中图片的setting较像
-   一些常见的物体更容易学习

# Null-text Inversion {#null-text-inversion heading="Null-text Inversion"}

![Pasted image
20240301204752.png](E:\Learning\MyObsidian\MyObsidian\Pasted%20image%2020240301204752.png){.internal-embed
touched="true"}\
Inversion: 通过优化uncond 中的null-text
