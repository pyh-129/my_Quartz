

# Collaborative
第一个关于：多个模态可能关注点不同，在各个时期影响也不同，如何更好利用这些模态之间互补的信息，他核心思想是对于每个模态训练一个dynamic diffuser（就是unet)，I influence function 衡量每个模态影响，结果是加权。
在文中他们用influence function观察各个模态对结果的变化，发现...
不仅适用于图片合成，编辑

缺陷：
+ 每一个模态都要去训练dynamic diffuser,对算力要求很高（4v100 )几天

Imagic: 
![[Pasted image 20240311154146.png]]
每edit一个图片都要fine-tune ?


2.
我看其他有一些他们利用了attention去达到控制效果 比如说gligen,SA CA之间加入

Realcompo motivation是通过cross-attention map观察到t2i l2i所着重的区域不一样，t2i: object细节，l2i:box外整体布局 , 他在推理时给两个模型一个系数 还有mask，并以一种梯度下降的方式动态调整，使得t2i更关注bouding内部 l2i更关注bounding外部




# InstanceDiff (Spatial control)
## Motivation
+ text control 不够精细->nstance-level control
	+ 每个instance language condition
	+ 指定方式：single points, scribbles, bounding boxes or intricate instance segmentation masks,


## Contribution
+ 一种简单的方式去融合给出的control
主要方法就是unifusion  scaleu, unifusion 处理instance-level condition, scaleu进一步提高性能
具体来讲给出全局的text caption（global text caption cg），n个instances caption (c,I),希望学习到这样的generation model. 对于Unifusion ，首先处理instance condition: 对于location转变为points,然后对于(c,I),用mlp处理  a CLIP text encoder τθ(·).  a Fourier mapping γ(·)

## Method
希望学习model:
	![[Pasted image 20240306175454.png]]
+   $c_g$: text
+  $(c_i,l_i)$: (caption,location)对n个instances

Unifusion : 处理每个instances condition，加在self-attention, cross attention之间
+ location
	+ locations : 2D points $p_i = {(x_k,y_k)}_{k=1}^n$ (对于instance i)
	+ $\gamma()$: Fourier mapping
	+ $\tau_{\theta}()$: CLIP text encoder
	+ 得到单个embedding : ![[Pasted image 20240306180803.png|220]],每个location format有不同MLP
+ Masked self-attention
	+ 原始的attention可能造成instance之间信息的泄露，所以用mask:
		![[Pasted image 20240306181620.png]]
		$I_{v_k}$指示的是 $v_k$是否在 instance i region中
![[Pasted image 20240306181959.png]]

ScaleU 

  参考 FreeU: 在去噪过程中，Unet skip conn 主要贡献高频特征，在推理阶段高频特征可能导致backbone去噪能力减弱。
  发现提高主干网络比例会提升效果，但是高频减弱会导致纹理过渡平滑。减少低频特征是为了解决由此带来纹理过渡平滑
•与FreeU不同的是：FreeU只用在了unet前两阶段和一半channel,而ScaleU用了全部stages和channels.
  ![[Pasted image 20240306212420.png]]







Multi-instance Sampler
为了在inference时候减少不同instances info leakage
![[Pasted image 20240306213054.png]]


# InstantID

CLIP embedding 认为他是弱对齐，语义信息是粗粒度的-> face encoder 对face ID问题有更强的identity feature

•.(以往将image features 和text features 合并丢到cross-attention layer->丢失特定信息，粗粒度）

和clip 特征空间 分开？
1. attention
2. self-attention: 较强
  prompt2prompt: 利用cross-attention control






instantID: 
针对关于人物ID生成，之前不够精细化
1. 用了flamingo中的 perceiver attention 进行iamge embedding (应该是处理high-resolution数据：from flamingo: a Perceiver-based [48] architecture that can produce a small fixed number of visual tokens per image/video, given a large and variable number of visual input features;Perceiver Resampler: from varying-size large feature maps to few visual tokens.)(Perceiver att 核心思想：核心思想是引入一些潜在单元（latent units，%% 最开始被随机初始化为可学习的参数 %%），形成注意力bottleneck，以消除Transformer的平方增长问题，并能够构建很深的网络。)



training - free : cow attention 

seed : 定制信号 attention  传播形式之间的差异 扩充30%pami
两种不同能力  （不同模型）不同控制方式  

aigc :