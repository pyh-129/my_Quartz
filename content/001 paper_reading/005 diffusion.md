Links:
1. [Lil log]( https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
2. [Awesome]( https://github.com/diff-usion/Awesome-Diffusion-Models)
3. [Survey](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)
4. [另一个整理](https://docs.qq.com/doc/DT2NmV1ZOYWRyaFh0)
https://sander.ai/2022/01/31/diffusion.html

![[Pasted image 20240116102623.png]]


# unified version
## Intro
1. generative model:
	1. given samples x -> learn true data distribution p(x)
	2. 哪些
		1. gan
		2. likelihood-based :assigns a high likelihood to the observed data samples.
			1. vae
		3. energy-based : a distribution is learned as an arbitrarily flexible energy function that is then normalized.
		4. score-based: instead of learning to model the energy function itself, they learn the score of the energy-based model as a neural network.
> quote from Yang Song
> ![[Pasted image 20240116102851.png]]
## BG

+ generated by latent variable $z$
+ generally learn low-dim latent representations
### ELBO

1. $p(x,z)$: model the latent variable and the data, likelihood-based: maximize the likelihood $p(x)$.
2. ![[Pasted image 20231129170724.png]]![[Pasted image 20231129170731.png]]


3. 推导evidence lower bound:（VLB ELB)![[Pasted image 20231129171149.png]]KL大于0, $q_\phi(z|x)$: the approximate posterior.p(x):likelihood of observed or generated data
	1. 当KL趋近于0意味着logp(x)接近于ELBO式。如果减小KL距离，则q接近真实后验分布。但因为我们不知道真实后验，所以KL很难计算。而左侧p(x)与参数$\phi$无关，所以optimize ELBO就等于使得估计的后验 接近真实的后验。并且因为KL趋近于0，可以用ELBO估计p(x)

### VAE
![[Pasted image 20240223102231.png]]
1. maximize ELBO![[Pasted image 20231129172419.png]]
	1. encoder learn $q$, decoder learn $p$
	+ 式子
		+ reconstruction likelihood of decoder:确保decoder对latent有效建模
		+ 学习到的 z 分布（variational distribution) （q(z|x))和先验相似程度。确保encoder有效建模而不是一个Dirac函数
2. encoder: 学习一个multi gaussion, 假设prior是standard gaussion:![[Pasted image 20231129173604.png]]
	1. reconstruction term 可以用monte varlo估计，latents ${z^{(l)}}^L_{l=1}$  are sampled from $q_\phi(z|x)$ .单纯sample是不可导的-> reparamterization trick
	2.  reparamterization trick: 将r.v.重写做noise variable函数，从而化为从一个standard gaussioan 中sample,能够对参数做gradient descent.(注意 $\odot$ element-wise product)![[Pasted image 20231129174306.png]]
	3. VAE中 z dim 小于x->学到compact latent vector, 可修改latent vector控制生成
	

### HVAE
将前向反向都看作为马尔科夫链： decoding each latent $z_t$ only conditions on previous latent $z_{t+1}$, 低级依赖高级

![[Pasted image 20231129205321.png]]
![[Pasted image 20231129205331.png]]
Lower-bound:
![[Pasted image 20231129205414.png]]


## Variational Diffusion Models
![[Pasted image 20231129205937.png]]
1. 与HVAE不同
	1. latent dim = data dim （所以latent 和data 都写作$x_t$ ） 
	2. at t, the latent encoder is defined as a linear Gaussian model
	3. The Gaussian parameter of latent encoder vary -> at final T, standard Gaussian
2. the distribution of each latent variable in the encoder is a Gaussian centered around its previous hierarchical latent. 
	1. ![[Pasted image 20231129210136.png]]
	2. linear Gaussian parameters: hyperparameters or learned 
	3. ![[Pasted image 20231130091907.png]]![[Pasted image 20231130091914.png]]
	4. ![[Pasted image 20231130091944.png]]![[Pasted image 20231130092041.png]]
	5. 不同于HVAE, encoder过程没有参数 $\phi$ ,每一步为Gaussian. 我们关心 $p_\theta(x_{t-1}|x_t)$ 以生成新的$x_0$.
3. ELBO式子
	1. ![[Pasted image 20231130093329.png]]
		1. reconstruction: 
		2. prior matching : 不用优化，没有learnable parameters, 足够大T可以使最后为gaussian, 这一项变为0
		3. consistency term: 因此主要优化第三个式子, 然而使用Monte Carlo涉及两个r.v. $x_{t-1},x_{t+1}$ , 会变得不稳定（这两个随机变量噪声叠加，最终估计的是加上所有采样，所以variance 会高）
		4. 由于$q(x_{t-1}|x_t) = \int q(x_0)q(x_{t-1}|x_t, x_0) dx_0$无法求（注：推导 :$q(x_{t-1}|x_t,x_0)$是 given $x_t,x_0$，如果将$q(x_{t-1}|x_t,x_0)$看作 $q(A|B)$ 那么后面就是边缘概率的积分） 利用markov化为另一种形式![[Pasted image 20231130094503.png]]![[Pasted image 20231130094516.png]]
				1. 最后一项：denosing matching. 希望denosing transition step $p_\theta(x_{t-1}|x_t)$ 和 ground-truth denosing transition step $q(x_{t-1}|x_t,x_0)$越接近越好。其中在HVAE由于本身encoder learning复杂因此$q(x_{t-1}|x_t,x_0)$不好计算，而在VDM中可以利用gaussian性质简化：
					1. ![[Pasted image 20231130095509.png]]  这样可以用markov, 只需要找到$q(x_{t-1}|x_0),q(x_t|x_0)$就可以。  
					2. ![[Pasted image 20231130095345.png]]![[Pasted image 20231130095444.png]]
					3. $q(x_{t-1}|x_t,x_0)$最终化为![[Pasted image 20231130095816.png]]
				2. 由于 希望denosing transition step $p_\theta(x_{t-1}|x_t)$ 和 ground-truth denosing transition step $q(x_{t-1}|x_t,x_0)$越接近越好，可以把$p_\theta$ 建模为Gaussian :![[Pasted image 20231130100706.png]], mean确定后参数确定，从而variance确定
					1. ![[Pasted image 20231130100921.png]]![[Pasted image 20231130100935.png]] $x_\theta$ 由 neural network给出
					2. ![[Pasted image 20231130101050.png]]
					3. ![[Pasted image 20231130101105.png]]
					4. 最后是minimize expectations over all timesteps![[Pasted image 20231130101225.png]]
4. Learning noise parameters
	1. ![[Pasted image 20231130101848.png]]由前面推导得SNR，SNR越小Noise越大，希望t增加snr单调减少。
	2. 最终可化简为![[Pasted image 20231130102053.png]]![[Pasted image 20231130102103.png]]
	3. 希望构建nn去model snr, 表达为![[Pasted image 20231130102148.png]]
5. noise形式
	1. 带入![[Pasted image 20231130102716.png]]到 $u_q$ , 得![[Pasted image 20231130102741.png]]
	2. 带入KL式![[Pasted image 20231130102803.png]]
	3. 表示predict x0相当于predict noise.(一些表明predict noise performance更好)
6. 形式
	1. Tweedie’s Formula : "states that the true mean of an exponential family distribution, given samples drawn from it, can be estimated by the maximum likelihood estimate of the samples (aka empirical mean) plus some correction term involving the score of the estimate." 矫正项考虑到sample分布情况![[Pasted image 20231130103552.png]]
	2. ![[Pasted image 20231130103710.png]]![[Pasted image 20231130103727.png]]
	3. 带入$u_q$ 推出: 
		1. ![[Pasted image 20231130103834.png]]![[Pasted image 20231130103849.png]]
	4. ![[Pasted image 20231130103917.png]]![[Pasted image 20231130103929.png]]$s_\theta(x_t,t)$ is a nn learns to predict the score function $\nabla_{x_t}logp(x_t)$ 
	5. 和原来$\epsilon_0$ 关系？：可见多一个scale项，score function 指导我们如何在数据空间中移动以优化（最大化）目标函数（这里是log p）的作用，intuitively，是添加source noise 的反方向
		1. ![[Pasted image 20231130104036.png]]
	 

## 总结
### 扩散

$$
X_t \sim  \mathcal{N}(\sqrt{\alpha_t}X_{t-1},(1-\alpha_t)I)

$$
$$X_t \sim  \mathcal{N}(\sqrt{\bar{\alpha_t}}X_{0},(1-\bar{\alpha_t})I)$$
$$X_t = \sqrt{\alpha_t}X_{t-1} + \sqrt{1-\alpha_t}Z_t$$
$$X_t = \sqrt{\bar{\alpha_t}}X_{0} + \sqrt{(1-\bar{\alpha_t})}Z$$
### 反向

$$X_{t-1} \sim \mathcal{N}(\mu,\sigma^2)$$ $$\mu =  \sqrt{\alpha_t} \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}X_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}X_0 =  \frac{1}{\sqrt{\alpha_t}}(X_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\widetilde Z)$$
$$\sigma^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$

![[Pasted image 20240223110641.png]]
注：
## score-based generative models
1. energy-based model:
	1. 任意 distribution表示：![[Pasted image 20231130105217.png]]其中$f_\theta$是energy function, model by nn
	2. $z_\theta$是normalization 项 ，很难计算，用log p计算梯度去除了normalization项（一个常数项）
	3. sample过程：![[Pasted image 20231130110043.png]] score function指示我们如何走直到一个mode.添加噪声$\epsilon$ 加入随机化，避免生成时总到一个mode
2. 挑战
	1. the score function is ill-defined when x lies on a low-dimensional manifold in a high-dimensional space. 自然图像从像素上来说是高维的，但处于低维流形中（往往符合内在一些规律，可以用低维表示核心信息。而不在低维流形的点是零概率，因此log不定义。
	2. 在low density regions 期望不准确。（即对于rarely seen 或unseen )
	3. Langevin dynamics 不能算混合的概率，log后丢失系数
		1. ![[Pasted image 20231130210559.png]]
		2. 混合高斯，尽管右下到达概率大,丢失系数后还是以同等概率到达
3. 方法：
	1. adding multiple levels of Gaussian noise to the data.
		1. 避免处于低维流形
		2. 对每个mode 增加了覆盖的地方，
		3. ？？” Lastly, adding multiple levels of Gaussian noise with increasing variance will result in intermediate distributions that respect the ground truth mixing coefficients.

## Guidence( Conditional )
![[Pasted image 20231201105353.png]]
y: a text encoding in image-text generation, or a low-resolution image 
learn:![[Pasted image 20231201105448.png]]
guidence: 普通方法训练conditional diffusion时会忽视给定条件信息，guidence提供更加准确的指导但以牺牲diversity为代价

### classifier guidance
1. ![[Pasted image 20231201110114.png]]
	1. unconditional score: 像之前一样learn
	2. adv gradient : 训练一个classifier, 输入$x_t$，输出 $y$
	3. sample时 ：计算两者和
2. ![[Pasted image 20231201112524.png]]
	1. 系数控制conditional的情况
3. 缺点：依赖于classifier, (因为需要处理任意的noisy inputs, pretrained model不能用，需要临时训练）

### classifier-free guidance

![[Pasted image 20231128210311.png]]

![[Pasted image 20231128210434.png]]


# VAE
![[Pasted image 20231128170255.png]]
Intuitive:
sigma 表示vatiance（exp()表示限制为正数，即可以表示为variance), e是从nomral distribution中sample,相乘加到原来的m上就是noise. 但是你单单训练reconstruction error是不行的，他会自动觉得variance越小越好(不会有不同code信息，reconstruction error最小），需要限制variance不可以太小

![[Pasted image 20231128170801.png]]

![[Pasted image 20231128172129.png]]

![[Pasted image 20231128172141.png]]

![[Pasted image 20231128172437.png]]

![[Pasted image 20231128172720.png]]

![[Pasted image 20231128172933.png]]

maximize likelihood 就是 最大Lb


![[Pasted image 20231128173818.png]]

VAE问题：
![[Pasted image 20231128174020.png]]
只是想模仿database里面的 没想产生新的


![[Pasted image 20231128174910.png]]
# GAN
![[Pasted image 20231128174533.png]]



问题：

不知道discriminator是不是对的，如果disc很好  可能是generator 太low

## SDE

score-based:
![[Pasted image 20240228103621.png]]
[sde理解](https://zhuanlan.zhihu.com/p/589466244)
1. sde将ddpm 和 score-based 统一一个框架，本质还是估计score
2. sde_based diffusion
	1. $$dx = f(x,t)dt+g(t)dw$$
			f(x,t): drift coefficient
			g(t) : diffusion coeffient
			w: Brownian motion
			![[Pasted image 20231231165341.png]]

	2. 重建
		1. $$x_{t+\Delta t} - x_t=f(x_t,t)+g(t) \sqrt{\Delta t}\varepsilon $$
		2. $$P(x_{t+\Delta t}|x_t) \sim N(x_t + f(x_t,t)\Delta t,g(t)\Delta t)$$
		3. 化log,泰勒一阶展开，$x_t$是关于t函数，求导会有两项
			注；多元函数泰勒：![[Pasted image 20231231170446.png]]
			![[Pasted image 20231231170617.png]]
			![[Pasted image 20231231171022.png]]
			![[Pasted image 20231231171754.png]]
			![[Pasted image 20231231171818.png]]
			![[Pasted image 20231231171913.png]]
			实际上是求score
		4. 
			扩散&重建：
			![[Pasted image 20231231172112.png]]

3. VE-SDE & VP-SDE
	1. ![[Pasted image 20231231172437.png]]
	2. 


## Q&A
1. 生成器通过某些“捷径”骗过判别器，导致效果并不好??
2. 1. **随机流（Stochastic Flow）**：在数学和统计学中，随机流是指随机变量的序列或过程，它们随时间或某个参数变化。这些流经常用于描述随时间演变的随机系统，例如金融市场中的股价变动、物理系统中的粒子运动等。在机器学习和深度学习中，随机流可以指由随机变量驱动的神经网络的行为或输出。
3. VAE
	1. 通过控制某一dim来控制输出（原来pixelrnn不可以吗
4. Diffusion
	1. ![[Pasted image 20231129210714.png]]