# 3.1
\[1] 表示layer (1)表示训练样本
![[Pasted image 20231117092300.png]]
![[Pasted image 20231117092642.png]]


(4,3):4 表示输出维度 3 输入feature
![[Pasted image 20231117093302.png]]

![[Pasted image 20231117212809.png]]
m samples:

竖向的是特征 横向是不同样本：
![[Pasted image 20231117213306.png]]![[Pasted image 20231117213626.png]]

向量化表示：
![[Pasted image 20231117213843.png]] 

## 激活函数

hidden layer 用tanh() 更好，因为均值为0，在hidden layer可能需要平移数据让数据均值为0

output layer 希望是 sigmoid()函数，因为希望在0-1而不是-1-1之间

问题：
1.尾部 梯度过小->ReLU函数 （在0处可以赋值0或1）
除了output layer 其他选ReLU更好
	![[Pasted image 20231117214535.png]]
	用ReLU比用tanh或sigmoid函数训练更快，因为没有接近0处的梯度过小的问题

激活函数导数
![[Pasted image 20231117215131.png]]
![[Pasted image 20231117215155.png]]

![[Pasted image 20231117215250.png]]

![[Pasted image 20231117215318.png]]


$n^{[0]}$ 特征数 $n^{[1]}$ 个隐藏单元 $n^{[2]}$个输出单元,有m个样本
![[Pasted image 20231117221306.png]]
![[Pasted image 20231117220327.png]]
keepdims=true确保输出是一个矩阵（如（n,1)而不是（n,))

![[Pasted image 20231117221853.png]]

注意乘号是elementwise product