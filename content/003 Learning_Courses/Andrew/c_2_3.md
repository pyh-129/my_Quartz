## 调试处理
超参：随机取值和精确搜索
随即均匀取值不合理，对数标尺搜索超参数更合理：（比如，0.0001到0.001之间会有更多的搜索资源可用）
**Python**：使`r=-4*np.random.rand()`，$\alpha = 10^r$
对于$\beta$: 同上线性值不合理 $\beta = 1-10^r$



## 为什么batch-norm 有效：
1. 对于输入，规范到一定范围加速学习
2. covariate shift: 分布改变需要重新学习![[Pasted image 20240118110456.png]]
	降低了前层参数更新会影响后层数值分布的程度，增加了稳定性。（BN可以避免前面层的参数变化导致激活变化过大，进而导致后面层不好学习的情况，如果出现coviriate shift比如更多色彩的猫，input 分布改变很大，它改变较小，即*减弱了前层参数与后层参数作用之前的联系，每层更独立学习从而加速学习过程*
3. 轻微正则化的作用
## 测试时BatchNorm


$\mu ,\sigma$ :estimate using exponentially weighted average (在mini-batch 上)
![[Pasted image 20240120162252.png]]

## Softmax
多元分类：输出是一个向量
![[Pasted image 20240120162456.png]]
之前的函数sigmoid/relu都是接受一个数值，softmax接受向量

C=2, softmax- > logistic regression

![[Pasted image 20240120163504.png]]
