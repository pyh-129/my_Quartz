# 2.2
1. logistic ![[Pasted image 20231107170758.png]]
2. loss 
	1. $x^i$ i-th sample![[Pasted image 20231107171604.png]]
	2. 不用squared error，会有优化问题，变成non-convex,会有很多local-minima![[Pasted image 20231107171039.png]]
	3. loss->single training example  cost function->cost of parameters![[Pasted image 20231107171623.png]]
## 2.4 Gradient
![[Pasted image 20231107171942.png]]


## 2.7 computational graph
![[Pasted image 20231107172247.png]]

## 2.9 logistic for gradient 
1. dz只是coding中变量名
	![[Pasted image 20231107173337.png]]


## 2.10 m samples
1. naive:
	![[Pasted image 20231107174229.png]]
	缺点：
	1. 2 for-loop : 1)samples 2) features


## 2.11 Vectorization
```
z=np.dot(z,x)+b
```

![[Pasted image 20231107174900.png]]广播机制：
![[Pasted image 20231107175121.png]]

for logistic:
![[Pasted image 20231107175504.png]]![[Pasted image 20231107175547.png]]


## 2.15 Broadcasting
![[Pasted image 20231107175818.png]]![[Pasted image 20231107175857.png]]
对行列同样有效
![[Pasted image 20231107180140.png]]
trick:
1. 初始化nx1向量而不是秩为1的矩阵
	1. 上面那样a外积本来应该是矩阵确是数字
		![[Pasted image 20231107180512.png]]
		![[Pasted image 20231107180711.png]]
 
	numpy
	c
	![[Pasted image 20231117091550.png]]
	![[Pasted image 20231117091608.png]]
	最大似然估计：iid
	![[Pasted image 20231117091921.png]]
